# -*- coding: utf-8 -*-
"""Fine-tuning an LLM w/ LoRA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M0iHxACeIsB3WknHicGAEfXme2kSS-Xa
"""

!pip -q uninstall -y transformers accelerate peft
!pip -q install "transformers==4.45.2" "accelerate==0.34.2" "peft==0.12.0" "datasets==2.21.0" "evaluate==0.4.2"

from datasets import load_dataset, DatasetDict, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
    pipeline
)
import torch
from torch.optim import Adam
import numpy as np

# Log in to Hugging Face. This is necessary for accessing Llama models.
# notebook_login()

model_id = "distilbert-base-uncased"
model_checkpoint = "distilbert-base-uncased"

# Check if a GPU is available and set the device accordingly
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

print(f"Model '{model_id}' loaded successfully on {device}.")

label2id = {'Negative': 0, 'Positive': 1}
id2label = {v: k for k, v in label2id.items()}

# Now your model instantiation code will work
model = AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id
)

model

# load dataset
dataset = load_dataset("shawhin/imdb-truncated")
dataset

# display % of training data with label=1
np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])

# create tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)

  # create tokenize function
def tokenize_function(examples):
      # extract text
      text = examples["text"]

      # tokenize and truncate text
      tokenizer.truncate_side = "left"
      tokenized_inputs = tokenizer(
          text,
          return_tensors="np",
          truncation=True,
          max_length=512
      )

      return tokenized_inputs

# add pad token if none exists
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

# tokenize training and validation datasets
tokenzied_dataset = dataset.map(tokenize_function, batched=True)
tokenzied_dataset

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

text_list = [
    "It was good.",
    "Not a fan, don't recommend.",
    "Better than the first one.",
    "This is not worth watching even once.",
    "This one is a pass."
]

print("Untrained model predictions:")
print("-----------------------")

with torch.no_grad():
    for text in text_list:
        # Use tokenizer(...) not encode, get a dict and move each tensor to device
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        outputs = model(**inputs)
        logits = outputs.logits
        pred_id = int(torch.argmax(logits, dim=-1))
        print(f"{text} - {id2label[pred_id]}")

from peft import LoraConfig, get_peft_model
peft_config = LoraConfig(task_type="SEQ_CLS", # sequence classification
                        r=4, # intrinsic rank of trainable weight matrix
                        lora_alpha=32, # this is like a learning rate
                        lora_dropout=0.01, # probability of dropout
                        target_modules = ['q_lin']) # we apply lora to query layer

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

from transformers import TrainingArguments

# hyperparameters
lr = 1e-3
batch_size = 4
num_epochs = 10
# define training arguments
training_args = TrainingArguments(
    output_dir= model_checkpoint + "-lora-text-classification",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    report_to=[],
    load_best_model_at_end=True,
)

tokenzied_dataset

# Check if you're using GPU
print(f"Device: {device}")
print(f"GPU available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU name: {torch.cuda.get_device_name(0)}")

print("Original tokenized columns:", tokenzied_dataset["train"].column_names)

# Define the function to rename 'label' to 'labels'
def rename_and_format(example):
    example['labels'] = example['label']
    return example

# Apply the renaming
tokenzied_dataset = tokenzied_dataset.map(rename_and_format)

# Remove the columns that the model doesn't need: 'text' and the old 'label'
cleaned_dataset = tokenzied_dataset.remove_columns(['label', 'text'])

print("Cleaned dataset columns:", cleaned_dataset["train"].column_names)

train = cleaned_dataset["train"]
eval = cleaned_dataset["validation"]

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train,
    eval_dataset=eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()

model.to('cuda')
print("Trained model predictions:")
print("------------------------")
for text in text_list:
    inputs = tokenizer.encode(text, return_tensors="pt").to("cuda")

    logits = model(inputs).logits
    predictions = torch.max(logits, 1).indices

    print(text + " - " + id2label[predictions.tolist()[0]])

